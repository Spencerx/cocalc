/*

1. turn off nats-server handling for the hub by sending this message from a browser as an admin:

   await cc.client.nats_client.hub.system.terminate({service:'changefeeds'})

2. Run this

   echo "require('@cocalc/database/nats/changefeeds').init()" | COCALC_MODE='single-user' DEBUG_CONSOLE=yes DEBUG=cocalc:* node

*/

import getLogger from "@cocalc/backend/logger";
import { JSONCodec } from "nats";
import userQuery from "@cocalc/database/user-query";
import { getConnection } from "@cocalc/backend/nats";
import { getUserId } from "@cocalc/nats/hub-api";
import { callback } from "awaiting";
import { db } from "@cocalc/database";
import {
  createSyncTable,
  CHANGEFEED_INTEREST_PERIOD_MS,
} from "@cocalc/nats/sync/synctable";
import { sha1 } from "@cocalc/backend/misc_node";
import jsonStableStringify from "json-stable-stringify";
import { reuseInFlight } from "@cocalc/util/reuse-in-flight";
import { uuid } from "@cocalc/util/misc";
import { delay } from "awaiting";
import type { Subscription } from "nats";
import { debounce } from "lodash";

const logger = getLogger("database:nats");

const DEBOUNCE_SAVE_TO_JETSTREAM = 100;
const MAX_TIME_SAVE_TO_JETSTREAM = 30000;

const jc = JSONCodec();

let subscription: Subscription | null = null;
export async function init() {
  const subject = "hub.*.*.db";
  logger.debug(`init -- subject='${subject}', options=`, {
    queue: "0",
  });
  const nc = await getConnection();
  subscription = nc.subscribe(subject, { queue: "0" });
  for await (const mesg of subscription) {
    handleRequest(mesg, nc);
  }
}

export function terminate() {
  logger.debug("terminating");
  subscription?.unsubscribe();
  // also, stop reporting data into the streams
  cancelAllChangefeeds();
}

async function handleRequest(mesg, nc) {
  let resp;
  try {
    const { account_id, project_id } = getUserId(mesg.subject);
    const { name, args } = jc.decode(mesg.data) ?? ({} as any);
    if (!name) {
      throw Error("api endpoint name must be given in message");
    }
    logger.debug("handling database request:", {
      account_id,
      project_id,
      name,
      //args,
    });
    resp = await getResponse({ name, args, account_id, project_id, nc });
  } catch (err) {
    resp = { error: `${err}` };
  }
  mesg.respond(jc.encode(resp));
}

async function getResponse({ name, args, account_id, project_id, nc }) {
  if (name == "userQuery") {
    const opts = { ...args[0], account_id, project_id };
    if (!opts.changes) {
      // a normal query
      return await userQuery(opts);
    } else {
      return await createChangefeed(opts, nc);
    }
  } else {
    throw Error(`name='${name}' not implemented`);
  }
}

function queryTable(query) {
  return Object.keys(query)[0];
}

const changefeedInterest: { [hash: string]: number } = {};
const changefeedHashes: { [id: string]: string } = {};

function cancelChangefeed(id) {
  logger.debug("cancelChangefeed", { id });
  const hash = changefeedHashes[id];
  if (!hash) {
    // already canceled
    return;
  }
  delete changefeedInterest[hash];
  delete changefeedHashes[id];
  db().user_query_cancel_changefeed({ id });
}

function cancelAllChangefeeds() {
  logger.debug("cancelAllChangefeeds");
  for (const id in changefeedHashes) {
    cancelChangefeed(id);
  }
}

// This is tricky.  We return the first result as a normal
// async function, but then handle (and don't return)
// the subsequent calls to cb generated by the changefeed.
const createChangefeed = reuseInFlight(
  async (opts, nc) => {
    const query = opts.query;
    const hash = sha1(jsonStableStringify(query));
    const now = Date.now();
    if (changefeedInterest[hash]) {
      changefeedInterest[hash] = now;
      logger.debug("using existing changefeed for", queryTable(query));
      return;
    }
    logger.debug("creating new changefeed for", queryTable(query));
    const changes = uuid();
    changefeedHashes[changes] = hash;
    const env = { nc, jc, sha1 };
    const synctable = createSyncTable({
      query,
      env,
      account_id: opts.account_id,
      project_id: opts.project_id,
      atomic: true,
    });
    await synctable.init();

    /*
    This code is complicated because it has to be.

    0. Extra work to avoid ever setting a key in the nats kv if
       we don't have to.  Nat's doesn't do anything to avoid broadcasting
       changes, so this is very valuable. For a supercluster it will
       be a critical optimization.

    1. The initial set could
       take a long time and still be happening as we get updates
       later.  Thus we MUST use a work queue to ensure that every
       update happens in the order it was received and also after
       the initial state is set.

    2. We keep a map in memory of the current value of all objects
       so that in the case of an *update* we do not have to read
       the last received value, which would take extra time and be
       particularly hard given the queue issue in 1.

    3. Saving to NATS could obviously fail intermittenly, e.g., if
       NATS is down for some reason or there are network issues.
       We retry with exponential backoff several times and
       finally give up... TODO: user is not informed about this yet.

    */

    // initalize map with exactly what is *currently* in the nats kv,
    // so we can be sure to never set anything we don't need to set.
    const map = await synctable.get(null, { natsKeys: true });
    const queue: {
      action: "insert" | "update" | "delete";
      obj: object;
    }[] = [];

    const deleteMap = (key, obj) => {
      if (map[key] !== undefined) {
        delete map[key];
        queue.push({ action: "delete", obj });
      }
    };

    const setMap = (key, obj) => {
      const cur = map[key];
      // always merge set
      const value = { ...cur, ...obj };
      // json so dates compare as strings.  Yes, we could make this faster, but this
      // is entirely in the server.
      if (JSON.stringify(cur) != JSON.stringify(value)) {
        map[key] = value;
        queue.push({ action: "update", obj: value });
      }
    };

    const synctableSetRows = async (rows) => {
      if (rows.length == 0) {
        return;
      }
      const v = rows.map(synctable.set);
      // wait for confirmation that sets are done
      let d = 2000;
      let t = 0;
      while (
        t < MAX_TIME_SAVE_TO_JETSTREAM &&
        changefeedHashes[changes] != null
      ) {
        const s = Date.now();
        try {
          await Promise.all(v);
          return;
        } catch (err) {
          logger.debug(`failed to save updates to NATS -- ${err}`);
          await delay(d);
          d = Math.min(10000, d * 1.3);
        }
        t += Date.now() - s;
      }
      logger.debug(
        "WARNING: couldn't save to NATS after many attempts -- we cancel this whole changefeed",
      );
      cancelChangefeed(changes);
    };

    const processQueue = debounce(
      reuseInFlight(async () => {
        const work = [...queue];
        // clear queue
        queue.length = 0;
        const rows: any[] = [];
        for (const { action, obj } of work) {
          if (action == "delete") {
            // if we hit a delete, we have to handle everything up
            // to this point, then do the delete.
            await synctableSetRows(rows);
            rows.length = 0;
            await synctable.delete(obj);
          } else {
            rows.push(obj);
          }
        }
        // handle anything left (will be everything if no deletes)
        await synctableSetRows(rows);
      }),
      DEBOUNCE_SAVE_TO_JETSTREAM,
      { leading: true, trailing: true },
    );

    const handleFirst = ({ cb, err, rows }) => {
      if (err || rows == null) {
        cb(err ?? "missing result");
        return;
      }
      for (const obj of rows) {
        setMap(synctable.getKey(obj), obj);
      }
      processQueue();
    };

    const handleUpdate = ({ action, new_val, old_val }) => {
      // action = 'insert', 'update', 'delete', 'close'
      // e.g., {"action":"insert","new_val":{"title":"testingxxxxx","project_id":"81e0c408-ac65-4114-bad5-5f4b6539bd0e"}}
      const key = synctable.getKey(new_val ?? old_val);
      if (action == "insert") {
        setMap(key, new_val);
      } else if (action == "update") {
        // update -- since atomic have to get the current value;
        // this of course assumes there is one process writing to
        // this part of the key value store (the atomic business).
        setMap(key, new_val);
      } else if (action == "delete") {
        deleteMap(key, old_val);
      } else if (action == "close") {
        cancelChangefeed(changes);
      }
      processQueue();
    };

    const f = (cb) => {
      let first = true;
      db().user_query({
        ...opts,
        changes,
        cb: (err, x) => {
          if (first) {
            first = false;
            handleFirst({ cb, err, rows: x?.[synctable.table] });
            return;
          }
          handleUpdate(x as any);
        },
      });
    };
    try {
      await callback(f);
      // it's running successfully
      changefeedInterest[hash] = Date.now();

      const watch = async () => {
        // it's all setup and running.  If there's no interest for a while, stop watching
        while (true) {
          await delay(CHANGEFEED_INTEREST_PERIOD_MS);
          if (
            Date.now() - changefeedInterest[hash] >
            CHANGEFEED_INTEREST_PERIOD_MS
          ) {
            logger.debug(
              "insufficient interest in the changefeed, so we stop it.",
              query,
            );
            cancelChangefeed(changes);
          }
        }
      };
      // do not block on this.
      watch();
      return;
    } catch (err) {
      // if anything goes wrong, make sure we don't think the changefeed is working.
      cancelChangefeed(changes);
      throw err;
    }
  },
  { createKey: (args) => jsonStableStringify(args[0]) },
);
